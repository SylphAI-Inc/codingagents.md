---
title: Agent Benchmarks
description: "Comparing AI coding agents — methodology, results, and community rankings"
---

## Coming Soon

We're preparing comprehensive benchmarks comparing AI coding agents across:

- **Code generation accuracy**
- **Multi-file editing**
- **Test writing quality**
- **Speed and cost efficiency**
- **Context handling on large codebases**

## Existing Benchmarks

- [SWE-bench](https://swebench.com/) — Real GitHub issues benchmark
- [Aider Leaderboard](https://aider.chat/docs/leaderboards/) — Code editing benchmarks
- [Vercel Agent Evals](https://vercel.com/blog/agents-md-outperforms-skills-in-our-agent-evals) — Framework-specific evals

## Contribute

Want to help? [Open an issue](https://github.com/SylphAI-Inc/coding_agent/issues) or submit a PR.

---

*[Edit this page](https://github.com/SylphAI-Inc/coding_agent/edit/main/src/content/docs/benchmarks/overview.md) · Managed by [Sylph.AI](https://sylph.ai)*
